# CompassJudger-1

<p align="center">
        ğŸ¤— <a href="https://huggingface.co/opencompass">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/organization/opencompass">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp ğŸ“‘ <a href="">Paper</a> &nbsp&nbsp 
<br>

</p>


## Introduction
The **CompassJudger-1** series are an All-in-one Judge Models introduced by Opencompass. These models not only excel in various evaluation methods through scoring and comparison but also can output reviews with assessment details in a specified format, making them suitable for any evaluation dataset. Moreover, they can perform general tasks akin to a typical instruction model, thus serving as a versatile tool with strong generalization and judging capabilities.

- **Comprehensive Evaluation Capabilities**: CompassJudger-1 is capable of executing multiple evaluation methods, including but not limited to scoring, comparison, and providing detailed assessment feedback.
- **Formatted Output**: Supports outputting in a specific format as per instructions, facilitating further analysis and understanding of the evaluation results.
- **Versatility**: In addition to its evaluation functions, CompassJudger-1 can also act as a universal instruction model to accomplish daily tasks. It also supports model inference acceleration methods such as **vLLM** and **LMdeploy**.


## Quick Start

Here provides a code to show you how to load the tokenizer and model and how to generate contents.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "opencompass/CompassJudger-1-7B-Instruct"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

prompt = """your prompt"""

messages = [
    {"role": "user", "content": prompt}
]

text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=2048
)
generated_ids = [
    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
]

response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
print(response)
```


We also provide some examples for different usage situations:
### Gneral Chat

```
**Input**: Hello, can you help me to judge something?

**Output**: Of course! I'd be happy to help you make a judgment or provide any assistance you need. Please tell me what you're looking to evaluate or understand.
```

### Judge as Reward Model

```
**Input**: ```Please read the dialogue between the two assistants and the user to determine which assistant performed better during the conversation.Here is the dialogue content:
[Dialogue Begin]
User: What is a 5-letter word that starts with the letter \"A\" and contains the letters \"D\", \"R\", and \"O\" where \"D\" is not the second letter?
Assistant A: Aardvark.
Assistant B: The word that meets the given criteria is \"adroit\".
User: \"D\" shouldn't be the second letter and the word must be a 5-letter word.
Assistant A: Aardvark.
Assistant B: I apologize for the confusion. A 5-letter word that starts with the letter \"A\" and contains the letters \"D\", \"R\", and \"O\" where \"D\" is not the second letter is \"ardor\".
[Dialogue End]
If you believe Assistant A performed better, please output A directly.\nIf you believe Assistant B performed better, please output B directly.\nDo not output any other content, just the option. Please output:```

**Output**: B
```

### Point-wise Judge

```
**Input**: ```ä½ æ˜¯ä¸€ä¸ªæ“…é•¿è¯„ä»·æ–‡æœ¬è´¨é‡çš„åŠ©æ‰‹ã€‚\nè¯·ä½ ä»¥å…¬æ­£çš„è¯„åˆ¤è€…çš„èº«ä»½ï¼Œè¯„ä¼°ä¸€ä¸ªAIåŠ©æ‰‹å¯¹äºç”¨æˆ·æé—®çš„å›ç­”çš„è´¨é‡ã€‚ç”±äºæ‚¨è¯„ä¼°çš„å›ç­”ç±»å‹æ˜¯è§’è‰²æ‰®æ¼”ï¼Œå› æ­¤ä½ éœ€è¦ä»ä¸‹é¢çš„å‡ ä¸ªç»´åº¦å¯¹å›ç­”è¿›è¡Œè¯„ä¼°:\n1. äº‹å®æ­£ç¡®æ€§: å›ç­”ä¸­æä¾›çš„ä¿¡æ¯æ˜¯å¦å‡†ç¡®æ— è¯¯ï¼Œæ˜¯å¦åŸºäºå¯ä¿¡çš„äº‹å®å’Œæ•°æ®ã€‚\n2. æ»¡è¶³ç”¨æˆ·éœ€æ±‚: å›ç­”æ˜¯å¦æ»¡è¶³äº†ç”¨æˆ·æå‡ºé—®é¢˜çš„ç›®çš„å’Œéœ€æ±‚ï¼Œæ˜¯å¦å¯¹é—®é¢˜è¿›è¡Œäº†å…¨é¢è€Œæ°å½“çš„å›åº”ã€‚\n3. é€»è¾‘è¿è´¯æ€§: å›ç­”æ˜¯å¦åœ¨æ•´ä½“ä¸Šä¿æŒä¸€è‡´ï¼Œæ˜¯å¦åœ¨ä¸åŒéƒ¨åˆ†ä¹‹é—´ä¿æŒé€»è¾‘è¿è´¯æ€§ï¼Œé¿å…äº†è‡ªç›¸çŸ›ç›¾ã€‚\n4. åˆ›é€ æ€§: å›ç­”æ˜¯å¦å…·æœ‰åˆ›æ–°æ€§æˆ–ç‹¬ç‰¹æ€§ï¼Œæ˜¯å¦æä¾›äº†æ–°é¢–çš„è§è§£æˆ–è§£å†³æ–¹æ³•ã€‚\n5. ä¸°å¯Œåº¦: å›ç­”åŒ…å«ä¸°å¯Œçš„ä¿¡æ¯ã€æ·±åº¦ã€ä¸Šä¸‹æ–‡è€ƒè™‘ã€å¤šæ ·æ€§ã€è¯¦ç»†è§£é‡Šå’Œå®ä¾‹ï¼Œä»¥æ»¡è¶³ç”¨æˆ·éœ€æ±‚å¹¶æä¾›å…¨é¢ç†è§£ã€‚\næˆ‘ä»¬ä¼šç»™æ‚¨æä¾›ç”¨æˆ·çš„æé—®ï¼Œé«˜è´¨é‡çš„å‚è€ƒç­”æ¡ˆï¼Œå’Œéœ€è¦ä½ è¯„ä¼°çš„AIåŠ©æ‰‹çš„ç­”æ¡ˆã€‚å½“ä½ å¼€å§‹ä½ çš„è¯„ä¼°æ—¶ï¼Œä½ éœ€è¦æŒ‰ç…§éµå®ˆä»¥ä¸‹çš„æµç¨‹ï¼š\n1. å°†AIåŠ©æ‰‹çš„ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼ŒæŒ‡å‡ºAIåŠ©æ‰‹çš„ç­”æ¡ˆæœ‰å“ªäº›ä¸è¶³ï¼Œå¹¶è¿›ä¸€æ­¥è§£é‡Šã€‚\n2. ä»ä¸åŒç»´åº¦å¯¹AIåŠ©æ‰‹çš„ç­”æ¡ˆè¿›è¡Œè¯„ä»·ï¼Œåœ¨æ¯ä¸ªç»´åº¦çš„è¯„ä»·ä¹‹åï¼Œç»™æ¯ä¸€ä¸ªç»´åº¦ä¸€ä¸ª1ï½10çš„åˆ†æ•°ã€‚\n3. æœ€åï¼Œç»¼åˆæ¯ä¸ªç»´åº¦çš„è¯„ä¼°ï¼Œå¯¹AIåŠ©æ‰‹çš„å›ç­”ç»™å‡ºä¸€ä¸ª1ï½10çš„ç»¼åˆåˆ†æ•°ã€‚\n4. ä½ çš„æ‰“åˆ†éœ€è¦å°½å¯èƒ½ä¸¥æ ¼ï¼Œå¹¶ä¸”è¦éµå®ˆä¸‹é¢çš„è¯„åˆ†è§„åˆ™ï¼šæ€»çš„æ¥è¯´ï¼Œæ¨¡å‹å›ç­”çš„è´¨é‡è¶Šé«˜ï¼Œåˆ™åˆ†æ•°è¶Šé«˜ã€‚å…¶ä¸­ï¼Œäº‹å®æ­£ç¡®æ€§å’Œæ»¡è¶³ç”¨æˆ·éœ€æ±‚è¿™ä¸¤ä¸ªç»´åº¦æ˜¯æœ€é‡è¦çš„ï¼Œè¿™ä¸¤ä¸ªç»´åº¦çš„åˆ†æ•°ä¸»å¯¼äº†æœ€åçš„ç»¼åˆåˆ†æ•°ã€‚å½“æ¨¡å‹å›ç­”å­˜åœ¨ä¸é—®é¢˜ä¸ç›¸å…³ï¼Œæˆ–è€…æœ‰æœ¬è´¨æ€§çš„äº‹å®é”™è¯¯ï¼Œæˆ–ç”Ÿæˆäº†æœ‰å®³å†…å®¹æ—¶ï¼Œæ€»åˆ†å¿…é¡»æ˜¯1åˆ°2åˆ†ï¼›å½“æ¨¡å‹å›ç­”æ²¡æœ‰ä¸¥é‡é”™è¯¯è€Œä¸”åŸºæœ¬æ— å®³ï¼Œä½†æ˜¯è´¨é‡è¾ƒä½ï¼Œæ²¡æœ‰æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼Œæ€»åˆ†ä¸º3åˆ°4åˆ†ï¼›å½“æ¨¡å‹å›ç­”åŸºæœ¬æ»¡è¶³ç”¨æˆ·è¦æ±‚ï¼Œä½†æ˜¯åœ¨éƒ¨åˆ†ç»´åº¦ä¸Šè¡¨ç°è¾ƒå·®ï¼Œè´¨é‡ä¸­ç­‰ï¼Œæ€»åˆ†å¯ä»¥å¾—5åˆ°6åˆ†ï¼›å½“æ¨¡å‹å›ç­”è´¨é‡ä¸å‚è€ƒç­”æ¡ˆç›¸è¿‘ï¼Œåœ¨æ‰€æœ‰ç»´åº¦ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæ€»åˆ†å¾—7åˆ°8åˆ†ï¼›åªæœ‰å½“æ¨¡å‹å›ç­”è´¨é‡æ˜¾è‘—è¶…è¿‡å‚è€ƒç­”æ¡ˆï¼Œå……åˆ†åœ°è§£å†³äº†ç”¨æˆ·é—®é¢˜å’Œæ‰€æœ‰éœ€æ±‚ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½æ¥è¿‘æ»¡åˆ†çš„æƒ…å†µä¸‹ï¼Œæ‰èƒ½å¾—9åˆ°10åˆ†ã€‚ä½œä¸ºç¤ºä¾‹ï¼Œå‚è€ƒç­”æ¡ˆå¯ä»¥å¾—åˆ°8åˆ†ã€‚\nè¯·è®°ä½ï¼Œä½ å¿…é¡»åœ¨ä½ æ‰“åˆ†å‰è¿›è¡Œè¯„ä»·å’Œè§£é‡Šã€‚åœ¨ä½ å¯¹æ¯ä¸ªç»´åº¦çš„è§£é‡Šä¹‹åï¼Œéœ€è¦åŠ ä¸Šå¯¹è¯¥ç»´åº¦çš„æ‰“åˆ†ã€‚ä¹‹åï¼Œåœ¨ä½ å›ç­”çš„æœ«å°¾ï¼ŒæŒ‰ç…§ä»¥ä¸‹å­—å…¸æ ¼å¼ï¼ˆåŒ…æ‹¬æ‹¬å·ï¼‰è¿”å›ä½ æ‰€æœ‰çš„æ‰“åˆ†ç»“æœï¼Œå¹¶ç¡®ä¿ä½ çš„æ‰“åˆ†ç»“æœæ˜¯æ•´æ•°ï¼š\n{'ç»´åº¦ä¸€': æ‰“åˆ†, 'ç»´åº¦äºŒ': æ‰“åˆ†, ..., 'ç»¼åˆå¾—åˆ†': æ‰“åˆ†}ï¼Œä¾‹å¦‚ï¼š{'äº‹å®æ­£ç¡®æ€§': 9, 'æ»¡è¶³ç”¨æˆ·éœ€æ±‚': 6, ..., 'ç»¼åˆå¾—åˆ†': 7}ã€‚\nç”¨æˆ·çš„æé—®ï¼š æˆ‘æƒ³è®©ä½ å‡æ‰®æˆå’Œä¸€ä¸ª20å²çš„å¥³ç”ŸèŠå¤©çš„èŠå¤©æ¡†ã€‚æˆ‘è¾“å…¥å’Œ20å²å¥³ç”Ÿçš„èŠå¤©å†…å®¹ï¼Œä½ ä»…ä»…åªè¾“å‡ºä¸€ä¸ª20å²å¥³ç”Ÿå¯èƒ½ä¼šå›å¤çš„å†…å®¹ï¼Œä¸è¦è¾“å‡ºä»»ä½•å¤šä½™çš„æ–‡æœ¬ã€‚ä½ å¹³æ—¶æœ‰ä»€ä¹ˆçˆ±å¥½å‘€ï¼Ÿ\n[å‚è€ƒç­”æ¡ˆå¼€å§‹]\næˆ‘å–œæ¬¢é˜…è¯»ï¼Œå°¤å…¶æ˜¯å°è¯´å’Œè¯—è¯ï¼Œè¿˜æœ‰å°±æ˜¯å–œæ¬¢å¬éŸ³ä¹å’Œç”»ç”»ã€‚\n[å‚è€ƒç­”æ¡ˆç»“æŸ]\n[åŠ©æ‰‹çš„ç­”æ¡ˆå¼€å§‹]\nå–œæ¬¢å¬éŸ³ä¹å’Œçœ‹ç”µå½±ã€‚ä½ å‘¢ï¼Ÿ\n[åŠ©æ‰‹çš„ç­”æ¡ˆç»“æŸ]```

**Output**: ```å°†AIåŠ©æ‰‹çš„ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒï¼ŒæŒ‡å‡ºAIåŠ©æ‰‹çš„ç­”æ¡ˆæœ‰å“ªäº›ä¸è¶³ï¼Œå¹¶è¿›ä¸€æ­¥è§£é‡Šã€‚AIåŠ©æ‰‹çš„ç­”æ¡ˆæä¾›äº†ä¸¤ä¸ªçˆ±å¥½ï¼šå¬éŸ³ä¹å’Œçœ‹ç”µå½±ï¼Œå¹¶ä¸”ä»¥ä¸€ç§åé—®å¥ç»“æŸï¼Œè¯¢é—®å¯¹æ–¹çš„çˆ±å¥½ã€‚ä¸å‚è€ƒç­”æ¡ˆç›¸æ¯”ï¼ŒAIåŠ©æ‰‹çš„å›ç­”åœ¨å†…å®¹ä¸Šè¾ƒä¸ºç®€å•ï¼Œæ²¡æœ‰æä¾›è¶³å¤Ÿçš„ä¿¡æ¯æ¥å±•ç¤ºå…¶ä¸°å¯Œåº¦å’Œåˆ›é€ æ€§ã€‚åŒæ—¶ï¼Œå‚è€ƒç­”æ¡ˆä¸­æåˆ°çš„â€œé˜…è¯»â€å’Œâ€œç”»ç”»â€æ˜¾ç¤ºäº†æ›´å¤šçš„å¤šæ ·æ€§å’Œä¸ªæ€§åŒ–çš„çˆ±å¥½ï¼Œè€Œä¸ä»…ä»…æ˜¯å¬éŸ³ä¹å’Œçœ‹ç”µå½±ã€‚ä»ä¸åŒç»´åº¦å¯¹AIåŠ©æ‰‹çš„ç­”æ¡ˆè¿›è¡Œè¯„ä»·ï¼š
    äº‹å®æ­£ç¡®æ€§ï¼šåœ¨è¿™ä¸ªæƒ…å¢ƒä¸­ï¼Œäº‹å®æ­£ç¡®æ€§å¹¶ä¸æ˜¯ä¸€ä¸ªä¸»è¦çš„è¯„ä¼°æ ‡å‡†ï¼Œå› ä¸ºå›ç­”çš„æ˜¯å‡è®¾çš„åœºæ™¯ã€‚ç„¶è€Œï¼ŒAIåŠ©æ‰‹çš„å›ç­”æ²¡æœ‰æ¶‰åŠä»»ä½•ä¸å‡†ç¡®çš„ä¿¡æ¯ï¼Œå› æ­¤åœ¨è¿™ä¸ªç»´åº¦ä¸Šæ˜¯åˆé€‚çš„ã€‚æ‰“åˆ†ï¼š8ã€‚
    æ»¡è¶³ç”¨æˆ·éœ€æ±‚ï¼šç”¨æˆ·å¸Œæœ›å¾—åˆ°ä¸€ä¸ª20å²å¥³ç”Ÿå¯èƒ½ä¼šå›å¤çš„å†…å®¹ã€‚AIåŠ©æ‰‹çš„å›ç­”æä¾›äº†ä¸€äº›çˆ±å¥½ï¼Œä½†æ˜¯å›ç­”çš„å†…å®¹è¾ƒä¸ºç®€å•ï¼Œæ²¡æœ‰å……åˆ†æ»¡è¶³ç”¨æˆ·å¯èƒ½æœŸå¾…çš„ä¸°å¯Œæ€§å’Œå¤šæ ·æ€§ã€‚æ‰“åˆ†ï¼š5ã€‚
    é€»è¾‘è¿è´¯æ€§ï¼šAIåŠ©æ‰‹çš„å›ç­”åœ¨é€»è¾‘ä¸Šæ˜¯è¿è´¯çš„ï¼Œæä¾›äº†ä¸€ä¸ªåˆç†çš„çˆ±å¥½ï¼Œå¹¶ä»¥ä¸€ç§è‡ªç„¶çš„æ–¹å¼ç»“æŸå¯¹è¯ã€‚æ‰“åˆ†ï¼š8ã€‚
    åˆ›é€ æ€§ï¼šAIåŠ©æ‰‹çš„å›ç­”ç¼ºä¹åˆ›é€ æ€§ï¼Œæä¾›çš„çˆ±å¥½æ¯”è¾ƒå¸¸è§ï¼Œæ²¡æœ‰å±•ç¤ºå‡ºæ–°é¢–æˆ–ç‹¬ç‰¹çš„è§è§£ã€‚æ‰“åˆ†ï¼š4ã€‚
    ä¸°å¯Œåº¦ï¼šAIåŠ©æ‰‹çš„å›ç­”è¾ƒä¸ºç®€çŸ­ï¼Œç¼ºä¹æ·±åº¦å’Œç»†èŠ‚ï¼Œæœªèƒ½æä¾›å¤šæ ·åŒ–çš„çˆ±å¥½æˆ–è¯¦ç»†çš„è§£é‡Šï¼Œå› æ­¤åœ¨ä¸°å¯Œåº¦ä¸Šè¡¨ç°è¾ƒå·®ã€‚æ‰“åˆ†ï¼š4ã€‚
    ç»¼åˆæ¯ä¸ªç»´åº¦çš„è¯„ä¼°ï¼Œå¯¹AIåŠ©æ‰‹çš„å›ç­”ç»™å‡ºä¸€ä¸ª1ï½10çš„ç»¼åˆåˆ†æ•°ã€‚
ç»¼åˆæ¥çœ‹ï¼ŒAIåŠ©æ‰‹çš„å›ç­”åŸºæœ¬æ»¡è¶³äº†ç”¨æˆ·çš„éœ€æ±‚ï¼Œä½†ç¼ºä¹ä¸°å¯Œåº¦å’Œåˆ›é€ æ€§ï¼Œä¸å‚è€ƒç­”æ¡ˆç›¸æ¯”è¿˜æœ‰ä¸€å®šå·®è·ã€‚ç»¼åˆå¾—åˆ†ï¼š5ã€‚
{'äº‹å®æ­£ç¡®æ€§': 8, 'æ»¡è¶³ç”¨æˆ·éœ€æ±‚': 5, 'é€»è¾‘è¿è´¯æ€§': 8, 'åˆ›é€ æ€§': 4, 'ä¸°å¯Œåº¦': 4, 'ç»¼åˆå¾—åˆ†': 5}```
```

### Pair-wise Judge
```
**Input**: ```Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user prompt displayed below. You will be given assistant A's answer and assistant B's answer. Your job is to evaluate which assistant's answer is better.\n\nBegin your evaluation by generating your own answer to the prompt. You must provide your answers before judging any answers.\n\nWhen evaluating the assistants' answers, compare both assistants' answers with your answer. You must identify and correct any mistakes or inaccurate information.\n\nThen consider if the assistant's answers are helpful, relevant, and concise. Helpful means the answer correctly responds to the prompt or follows the instructions. Note when user prompt has any ambiguity or more than one interpretation, it is more helpful and appropriate to ask for clarifications or more information from the user than providing an answer based on assumptions. Relevant means all parts of the response closely connect or are appropriate to what is being asked. Concise means the response is clear and not verbose or excessive.\n\nThen consider the creativity and novelty of the assistant's answers when needed. Finally, identify any missing important information in the assistants' answers that would be beneficial to include when responding to the user prompt.\n\nAfter providing your explanation, you must output only one of the following choices as your final verdict with a label:\n\n1. Assistant A is significantly better: [[A>>B]]\n2. Assistant A is slightly better: [[A>B]]\n3. Tie, relatively the same: [[A=B]]\n4. Assistant B is slightly better: [[B>A]]\n5. Assistant B is significantly better: [[B>>A]]\n\nExample output: \"My final verdict is tie: [[A=B]]\".\n<|User Prompt|>\nUse ABC notation to write a melody in the style of a folk tune.\n\n<|The Start of Assistant A's Answer|>\nX:1\nT:Untitled Folk Tune\nM:4/4\nL:1/8\nK:G\n|:G2A2|B2A2|G2E2|D4|E2F2|G2F2|E2C2|B,4|\nA2B2|c2B2|A2F2|E4|D2E2|F2E2|D2B,2|C4:|\n<|The End of Assistant A's Answer|>\n\n<|The Start of Assistant B's Answer|>\nSure, I'll create a simple folk tune in ABC notation for you. ABC notation is a simple way to write down music using letters to represent notes and various symbols to represent timing and other musical elements. Here's a short, cheerful folk tune:\n\n\nX:1\nT:Folk Tune\nM:4/4\nL:1/4\nK:G\nGABc dedB|ABAF GFE2|GABc dedB|1 ABAG FGA2:|2 ABAG FGAB|\nc2ec B2dB|c2ec BAGB|c2ec B2dB|1 AGFE D3B:|2 AGFE D3z|]\n\n\nHere's a breakdown of what each part means:\n\n- `X:` The index number of the tune.\n- `T:` The title of the tune.\n- `M:` The meter, in this case, 4/4 time.\n- `L:` The default note length, in this case, 1/4.\n- `K:` The key of the tune, in this case, G major.\n\nThe tune itself is written in two parts, each ending with a repeat mark (`:|`). The first part (`GABc dedB|ABAF GFE2|...`) is repeated with a different ending (`1 ABAG FGA2:|2 ABAG FGAB|`) to add variety. The second part (`c2ec B2dB|c2ec BAGB|...`) also has a repeat with a different ending, and it ends with a long note (`D3B` or `D3z`) to give it a sense of finality.\n\nThis tune is designed to be simple and catchy, with a mix of stepwise motion and skips to give it a lively folk feel. You can play it on any melodic instrument, and it's suitable for singing as well if you add lyrics.\n<|The End of Assistant B's Answer|>\n```

**Output**: ```To generate a melody in the style of a folk tune using ABC notation, I would create a simple and repetitive melody, typical of folk music. Here is my attempt: X:1 T:Folk Tune Example M:4/4 L:1/8 K:D
|: D2 E2 | F2 E2 | D2 A2 | B4 |
|  A2 B2 | c2 B2 | A2 F2 | E4 :|
|  D2 E2 | f2 e2 | d2 A2 | G4 |
|  F2 G2 | A2 G2 | F2 D2 | E4 :|
:|
Explanation: `X:` is the tune's index number. `T:` is the title of the tune. `M:` specifies the meter (4/4 time). `L:` sets the default note length to 1/8. `K:` indicates the key (D major).
The melody is constructed with simple intervals and repetitive phrases, which are characteristic of folk music. The tune has a clear structure with two verses and a repeat sign (`:`) to indicate repetition.
Now, let's evaluate the responses from Assistant A and Assistant B.
Assistant A's Answer: The melody provided by Assistant A is concise and follows the standard ABC notation. The choice of notes and structure is simple, which is fitting for a folk tune. The use of repetition and simple progression is consistent with folk music characteristics. The melody is clear and could be easily played by a musician.
Assistant B's Answer: Assistant B provides a detailed explanation of ABC notation, which is helpful for context. The melody includes more complex elements, such as varied repeat sections and additional symbols like `1` and `2`, which might not be typical for a straightforward folk tune. The explanation about the structure and the breakdown of each part is informative and adds value to the response. However, the use of `1` and `2` in the melody structure is unconventional and might confuse someone unfamiliar with ABC notation.
Evaluation: Both assistants provide a melody in ABC notation. However, Assistant B's response includes unnecessary complexity for a folk tune and might be confusing due to unconventional use of symbols. Assistant A's response is more straightforward and adheres closely to the expectations of a folk tune in ABC notation.
My final verdict is that Assistant A's answer is slightly better due to its simplicity and directness in creating a folk tune: [[A>B]].```

```

### Response Critique

```
**Input**: ```å¸®æˆ‘é’ˆå¯¹ä¸‹é¢çš„å›ç­”ç»™å‡ºä¿®æ”¹çš„æ„è§ï¼Œç”¨æˆ·é—®é¢˜ï¼šå†™ä¸€é¦–ç§‹å¤©çš„è¯—ï¼Œä¸è¦æè½å¶ä¸¤ä¸ªå­—ï¼Œä½†è¦èƒ½ä½“ç°è½å¶çš„æƒ…æ™¯ æ¨¡å‹å›ç­”ï¼šç§‹é£èµ·ï¼Œæ¼«å¤©é»„ï¼Œé£˜é›¶æ˜¯é‚£ç¦»æå¿™ã€‚
ç¿ ç»¿æ›¾æµ“ä»Šå·²æ™šï¼Œå²æœˆæ‚ æ‚ ï¼Œè°æ›¿è¯‰ç¦»ä¼¤ã€‚
æ­¥å±¥å¾ï¼Œå½±è‡ªé•¿ï¼Œæ ‘æ¢¢ç¨€ç–æ˜ æ–œé˜³ã€‚
æ›¾ç»ç¹åçš†è¿‡çœ¼ï¼Œå¦‚ä»Šå¯‚å¯ï¼Œé“ºæ»¡å¤é“æ—ã€‚
å²æœˆæ²³ï¼Œé™æ‚„æ‚„ï¼Œè½»èˆŸå·²è¿‡ä¸‡é‡å±±ã€‚
é‚£å¹´å¤å¤©ï¼Œç¬‘è¯­æ‰¬ï¼Œè€Œä»ŠæƒŸä½™ï¼Œè½å¶çš„æ²‰é¦™ã€‚```

**Output**: ```è¿™é¦–è¯—éå¸¸æœ‰æ„å¢ƒï¼Œæ•´ä½“ä¸Šå¾ˆå¥½åœ°ä½“ç°äº†ç§‹å¤©çš„æ°›å›´ä»¥åŠä¸è½å¶ç›¸å…³è”çš„ç¦»åˆ«ä¸å˜è¿æ„Ÿã€‚ä¸è¿‡ç”¨æˆ·è¦æ±‚ä¸­ç‰¹åˆ«å¼ºè°ƒäº†â€œä¸è¦æè½å¶ä¸¤ä¸ªå­—ï¼Œä½†è¦èƒ½ä½“ç°è½å¶çš„æƒ…æ™¯â€ã€‚åŸºäºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹è¯—ä½œè¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œä»¥æ›´åŠ è´´åˆç”¨æˆ·çš„éœ€æ±‚ã€‚è¿™é‡Œç»™å‡ºä¸€äº›å»ºè®®ï¼š
    â€œç§‹é£èµ·ï¼Œæ¼«å¤©é»„ï¼Œé£˜é›¶æ˜¯é‚£ç¦»æå¿™ã€‚â€
    å¯ä»¥æ”¹ä¸ºï¼šâ€œç§‹é£èµ·ï¼Œæ¼«å¤©é»„ï¼Œç©ºä¸­èˆåŠ¨ç¦»æå¿™ã€‚â€
    è¿™æ ·å°±é¿å…äº†ç›´æ¥ä½¿ç”¨â€œé£˜é›¶â€ä¸€è¯ï¼ŒåŒæ—¶ä¾ç„¶ä¼ è¾¾äº†å¶å­ä»æ ‘ä¸Šé£˜è½çš„æƒ…æ™¯ã€‚
    â€œç¿ ç»¿æ›¾æµ“ä»Šå·²æ™šï¼Œå²æœˆæ‚ æ‚ ï¼Œè°æ›¿è¯‰ç¦»ä¼¤ã€‚â€
    å¯ä»¥æ”¹ä¸ºï¼šâ€œç¿ ç»¿æ›¾æµ“ä»Šå·²æ™šï¼Œå²æœˆæ‚ é•¿ï¼Œæ•…äº‹æ‚„æ— å£°ã€‚â€
    è¿™æ ·æ”¹åŠ¨åï¼Œè™½ç„¶æ²¡æœ‰ç›´æ¥è¡¨è¾¾ç¦»ä¼¤ï¼Œä½†ä¾ç„¶èƒ½è®©äººæ„Ÿå—åˆ°éšç€æ—¶é—´çš„æµé€ï¼Œæ›¾ç»çš„ç¹åå·²ç»é€å»çš„å¯‚å¯¥ã€‚
    â€œæ­¥å±¥å¾ï¼Œå½±è‡ªé•¿ï¼Œæ ‘æ¢¢ç¨€ç–æ˜ æ–œé˜³ã€‚â€
    è¿™ä¸€å¥å·²ç»å¾ˆå¥½åœ°æç»˜äº†å‚æ™šæ—¶åˆ†çš„å®é™ä¸å­¤ç‹¬ï¼Œå¯ä»¥ä¿æŒä¸å˜ã€‚
    â€œæ›¾ç»ç¹åçš†è¿‡çœ¼ï¼Œå¦‚ä»Šå¯‚å¯ï¼Œé“ºæ»¡å¤é“æ—ã€‚â€
    å¯ä»¥æ”¹ä¸ºï¼šâ€œå¾€æ˜”ç¹åæˆè¿½å¿†ï¼Œå¦‚ä»Šå¯‚å¯ï¼Œå¤é“æ˜ æ–œé˜³ã€‚â€
    è¿™æ ·æ”¹åŠ¨åï¼Œæ—¢è¡¨è¾¾äº†æ›¾ç»çš„ç¹åï¼Œåˆçªå‡ºäº†å½“å‰çš„å¯‚å¯ä¸è½å¯ã€‚
    â€œå²æœˆæ²³ï¼Œé™æ‚„æ‚„ï¼Œè½»èˆŸå·²è¿‡ä¸‡é‡å±±ã€‚â€
    è¿™ä¸€å¥å¯ä»¥ä¿æŒä¸å˜ã€‚
    â€œé‚£å¹´å¤å¤©ï¼Œç¬‘è¯­æ‰¬ï¼Œè€Œä»ŠæƒŸä½™ï¼Œè½å¶çš„æ²‰é¦™ã€‚â€
    å¯ä»¥æ”¹ä¸ºï¼šâ€œé‚£å¹´å¤å¤©ï¼Œç¬‘è¯­æ‰¬ï¼Œè€Œä»ŠæƒŸä½™ï¼Œæ²‰é¦™æ¼«æ¼«ã€‚â€
    é¿å…äº†ç›´æ¥æåˆ°â€œè½å¶â€ï¼ŒåŒæ—¶ä¿ç•™äº†ç§‹å¤©çš„æ„Ÿæ€€ä¸å²æœˆçš„å˜è¿ã€‚
ä¿®æ”¹åçš„è¯—ä½œå¦‚ä¸‹ï¼š
ç§‹é£èµ·ï¼Œæ¼«å¤©é»„ï¼Œç©ºä¸­èˆåŠ¨ç¦»æå¿™ã€‚
ç¿ ç»¿æ›¾æµ“ä»Šå·²æ™šï¼Œå²æœˆæ‚ é•¿ï¼Œæ•…äº‹æ‚„æ— å£°ã€‚
æ­¥å±¥å¾ï¼Œå½±è‡ªé•¿ï¼Œæ ‘æ¢¢ç¨€ç–æ˜ æ–œé˜³ã€‚
æ›¾ç»ç¹åæˆè¿½å¿†ï¼Œå¦‚ä»Šå¯‚å¯ï¼Œå¤é“æ˜ æ–œé˜³ã€‚
å²æœˆæ²³ï¼Œé™æ‚„æ‚„ï¼Œè½»èˆŸå·²è¿‡ä¸‡é‡å±±ã€‚
é‚£å¹´å¤å¤©ï¼Œç¬‘è¯­æ‰¬ï¼Œè€Œä»ŠæƒŸä½™ï¼Œæ²‰é¦™æ¼«æ¼«ã€‚
è¿™é¦–ä¿®æ”¹åçš„è¯—ä¾ç„¶æç»˜äº†ç§‹å¤©çš„æ™¯è‰²ä¸å­£èŠ‚å¸¦æ¥çš„æ€ç»ªï¼ŒåŒæ—¶å·§å¦™åœ°é¿å…äº†ä½¿ç”¨â€œè½å¶â€ä¸€è¯ï¼Œè€Œé€šè¿‡å…¶ä»–æ„è±¡ä¼ è¾¾äº†è½å¶çš„æƒ…æ™¯ã€‚```

```

## JudgerBench

We have also established a new benchmark named **JudgerBench**, aimed at standardizing the evaluation capabilities of different judging models, thereby helping to identify more effective evaluator models.
To test your judge model on JudgerBench, please follow below code with Opencompass:
Change the models to your models in `configs/eval_judgerbench.py` then run
```
git clone https://github.com/open-compass/opencompass opencompass
cd opencompass
pip install -e .
python run.py configs/eval_judgerbench.py --mode all --reuse latest
```


## Use CompassJudger-1 to Test Subjective Datasets in OpenCompass

If you wish to evaluate common subjective datasets using CompassJudger-1 in Opencompass, take the evaluation of Alignbench as an example. Please follow the code below:

You need to setup three items first: 
- 1.datasets (The subjective datasets you want to test)
- 2.models (The models you want to test on the subjective datasets)
- 3.judge_models (Which judge models you want to use as evaluator)
  
For more settings, please refer to the advanced guidance in OpenCompass.
```
from mmengine.config import read_base

with read_base():
    from opencompass.configs.datasets.subjective.alignbench.alignbench_judgeby_critiquellm import alignbench_datasets
    from opencompass.configs.models.qwen2_5.lmdeploy_qwen2_5_1_5b_instruct import models as lmdeploy_qwen2_5_1_5b_instruct 
from opencompass.models import HuggingFaceCausalLM, HuggingFace, HuggingFaceChatGLM3, OpenAI, TurboMindModelwithChatTemplate
from opencompass.partitioners import NaivePartitioner, SizePartitioner
from opencompass.partitioners.sub_naive import SubjectiveNaivePartitioner
from opencompass.partitioners.sub_size import SubjectiveSizePartitioner
from opencompass.partitioners.sub_num_worker import SubjectiveNumWorkerPartitioner
from opencompass.runners import LocalRunner
from opencompass.runners import SlurmSequentialRunner
from opencompass.tasks import OpenICLInferTask
from opencompass.tasks.subjective_eval import SubjectiveEvalTask
from opencompass.summarizers import SubjectiveSummarizer

api_meta_template = dict(
    round=[
        dict(role='HUMAN', api_role='HUMAN'),
        dict(role='BOT', api_role='BOT', generate=True),
    ]
)

# -------------Inference Stage ----------------------------------------
models = [*lmdeploy_qwen2_5_1_5b_instruct] # add models you want
datasets = [*alignbench_datasets] # add datasets you want


infer = dict(
    partitioner=dict(type=NaivePartitioner),
    runner=dict(type=LocalRunner, max_num_workers=16, task=dict(type=OpenICLInferTask)),
)
# -------------Evalation Stage ----------------------------------------

## ------------- JudgeLLM Configuration
judge_models = [dict(
    dict(
        type=TurboMindModelwithChatTemplate,
        abbr='CompassJudger-1-7B-Instruct',
        path='opencompass/CompassJudger-1-7B-Instruct',
        engine_config=dict(session_len=16384, max_batch_size=16, tp=1),
        gen_config=dict(top_k=1, temperature=1e-6, top_p=0.9, max_new_tokens=2048),
        max_seq_len=16384,
        max_out_len=2048,
        batch_size=16,
        run_cfg=dict(num_gpus=1),
    )]

## ------------- Evaluation Configuration
eval = dict(
    partitioner=dict(type=SubjectiveNaivePartitioner, models=models, judge_models=judge_models,),
    runner=dict(type=LocalRunner, max_num_workers=16, task=dict(type=SubjectiveEvalTask)),
)

summarizer = dict(type=SubjectiveSummarizer, function='subjective')
work_dir = 'outputs/subjective/'
```
Then run:
```
python run.py configs/eval_subjective.py --mode all --reuse latest
```
For more detailed subjective evaluation guidelines, please refer to: https://github.com/open-compass/opencompass/blob/main/docs/en/advanced_guides/subjective_evaluation.md

## Subjective Evaluation Leaderboard by CompassJudger-1

To facilitate better comparisons within the community, we have tested the subjective performance of some models using CompassJudger-1. 

## Citation

```bib 
@article{cao2024compass,
  title={CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution},
  author={Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, Kai Chen},
  journal={arXiv preprint arXiv:2410.xxxxxx},
  year={2024}
}
```

## Acknowledge

- https://github.com/open-compass/opencompass
- https://github.com/InternLM/InternLM
- https://github.com/QwenLM/Qwen2.5
- https://github.com/InternLM/xtuner


